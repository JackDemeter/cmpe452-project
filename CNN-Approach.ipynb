{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_num_list(string):\n",
    "    return [int(num) for num in string.strip(\" []\").split(',')]\n",
    "\n",
    "def string_to_list(string):\n",
    "    return string.strip(\" []\").split(',')\n",
    "\n",
    "data['vector'] = data['vector'].apply(string_to_num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OH Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine max width\n",
    "width = max(data['NumberOfWords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to run this will brick your comp (most likely). Instead you can try to do OH vectorization at runtime.\n",
    "def OH_vector(vector_list):\n",
    "    global width\n",
    "    vocab = 7774\n",
    "    OH_matrix = np.zeros((vocab,width), dtype=np.float32)\n",
    "    for index,vector in enumerate(vector_list):\n",
    "        OH_matrix[vector][index]=1\n",
    "#     OH_matrix = np.expand_dims(OH_matrix, axis=0)\n",
    "    OH_matrix = np.expand_dims(OH_matrix, axis=-1)\n",
    "    return OH_matrix\n",
    "\n",
    "train = data[:10000]\n",
    "train_x = train['vector'].apply(OH_vector)\n",
    "train_y = train['BooleanSentiment']\n",
    "test = data[10000:11000]\n",
    "test_x = test['vector'].apply(OH_vector)\n",
    "test_y = test['BooleanSentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7774, 58, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = np.array(test_x.to_list())\n",
    "train_x = np.array(train_x.to_list())\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = np.array(test_y.to_list()).astype(np.float32)*0.99999999999\n",
    "train_y = np.array(train_y.to_list()).astype(np.float32)*0.99999999999\n",
    "test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 1, 54, 32)         1243872   \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 1, 18, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,280,865\n",
      "Trainable params: 1,280,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Convolutional layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (7774, 5), activation='relu', input_shape=(7774, 58,1)))\n",
    "model.add(layers.MaxPooling2D((1, 3)))\n",
    "\n",
    "# Dense/FC layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 76s 242ms/step - loss: 0.8105 - accuracy: 0.6109 - val_loss: 0.5271 - val_accuracy: 0.7380\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 74s 236ms/step - loss: 0.2789 - accuracy: 0.9171 - val_loss: 0.7192 - val_accuracy: 0.7310\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 74s 236ms/step - loss: 0.0368 - accuracy: 0.9948 - val_loss: 0.9828 - val_accuracy: 0.7400\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 75s 241ms/step - loss: 0.0124 - accuracy: 0.9991 - val_loss: 1.2148 - val_accuracy: 0.7450\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 76s 243ms/step - loss: 0.0135 - accuracy: 0.9991 - val_loss: 1.2558 - val_accuracy: 0.7410\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 75s 239ms/step - loss: 0.0178 - accuracy: 0.9988 - val_loss: 1.1261 - val_accuracy: 0.7410\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 74s 235ms/step - loss: 0.0156 - accuracy: 0.9989 - val_loss: 1.2333 - val_accuracy: 0.7380\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 74s 235ms/step - loss: 0.0167 - accuracy: 0.9988 - val_loss: 1.2966 - val_accuracy: 0.7410\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 79s 251ms/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 1.2970 - val_accuracy: 0.7330\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 74s 238ms/step - loss: 0.0074 - accuracy: 0.9995 - val_loss: 1.4726 - val_accuracy: 0.7340\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_x, train_y, epochs=10, \n",
    "                    validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>LowerTweet</th>\n",
       "      <th>BooleanSentiment</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>vector</th>\n",
       "      <th>NumberOfWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "      <td>1</td>\n",
       "      <td>['advice', 'talk', 'to', 'your', 'neighbours',...</td>\n",
       "      <td>['advic', 'talk', 'to', 'your', 'neighbour', '...</td>\n",
       "      <td>[593, 452, 1, 32, 1470, 184, 1, 2061, 805, 388...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>coronavirus australia woolworths to give elder...</td>\n",
       "      <td>1</td>\n",
       "      <td>['coronavirus', 'australia', 'woolworths', 'to...</td>\n",
       "      <td>['coronaviru', 'australia', 'woolworth', 'to',...</td>\n",
       "      <td>[8, 674, 1499, 1, 220, 315, 962, 1324, 33, 209...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "      <td>1</td>\n",
       "      <td>['my', 'food', 'stock', 'is', 'not', 'the', 'o...</td>\n",
       "      <td>['my', 'food', 'stock', 'is', 'not', 'the', 'o...</td>\n",
       "      <td>[38, 16, 60, 10, 34, 0, 118, 12, 203, 10, 183,...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>me ready to go at supermarket during the covid...</td>\n",
       "      <td>0</td>\n",
       "      <td>['me', 'ready', 'to', 'go', 'at', 'supermarket...</td>\n",
       "      <td>['me', 'readi', 'to', 'go', 'at', 'supermarket...</td>\n",
       "      <td>[88, 688, 1, 40, 19, 21, 66, 0, 3, 163, 34, 10...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>as news of the regions first confirmed covid c...</td>\n",
       "      <td>1</td>\n",
       "      <td>['as', 'news', 'of', 'the', 'regions', 'first'...</td>\n",
       "      <td>['as', 'new', 'of', 'the', 'region', 'first', ...</td>\n",
       "      <td>[27, 80, 4, 0, 1357, 210, 864, 3, 251, 862, 45...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>#Coronavirus tip: shop in your local Asian sup...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>coronavirus tip shop in your local asian supe...</td>\n",
       "      <td>1</td>\n",
       "      <td>['coronavirus', 'tip', 'shop', 'in', 'your', '...</td>\n",
       "      <td>['coronaviru', 'tip', 'shop', 'in', 'your', 'l...</td>\n",
       "      <td>[8, 368, 33, 7, 32, 110, 1119, 21, 525, 108, 1...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>Saudi Arabia will nearly double its debt ceili...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>saudi arabia will nearly double its debt ceili...</td>\n",
       "      <td>0</td>\n",
       "      <td>['saudi', 'arabia', 'will', 'nearly', 'double'...</td>\n",
       "      <td>['saudi', 'arabia', 'will', 'nearli', 'doubl',...</td>\n",
       "      <td>[763, 1145, 36, 1228, 790, 14, 834, 5912, 1, 1...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>I went to put a few bits in Food Bank collecti...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>i went to put a few bits in food bank collecti...</td>\n",
       "      <td>0</td>\n",
       "      <td>['went', 'to', 'put', 'few', 'bits', 'in', 'fo...</td>\n",
       "      <td>['went', 'to', 'put', 'few', 'bit', 'in', 'foo...</td>\n",
       "      <td>[253, 1, 190, 298, 684, 7, 16, 181, 731, 19, 0...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>It's a novel experience watching a government ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>it s a novel experience watching a government ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['it', 'novel', 'experience', 'watching', 'gov...</td>\n",
       "      <td>['it', 'novel', 'experi', 'watch', 'govern', '...</td>\n",
       "      <td>[14, 1059, 707, 407, 168, 67, 4379, 7, 378, 59]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>Fresh Thyme grocery store is giving employees ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>fresh thyme grocery store is giving employees ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['fresh', 'thyme', 'grocery', 'store', 'is', '...</td>\n",
       "      <td>['fresh', 'thyme', 'groceri', 'store', 'is', '...</td>\n",
       "      <td>[567, 26, 18, 10, 220, 157, 424, 2189, 17, 3, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      OriginalTweet  \\\n",
       "0              0  advice Talk to your neighbours family to excha...   \n",
       "1              1  Coronavirus Australia: Woolworths to give elde...   \n",
       "2              2  My food stock is not the only one which is emp...   \n",
       "3              3  Me, ready to go at supermarket during the #COV...   \n",
       "4              4  As news of the regionÂs first confirmed COVID...   \n",
       "...          ...                                                ...   \n",
       "9995        9995  #Coronavirus tip: shop in your local Asian sup...   \n",
       "9996        9996  Saudi Arabia will nearly double its debt ceili...   \n",
       "9997        9997  I went to put a few bits in Food Bank collecti...   \n",
       "9998        9998  It's a novel experience watching a government ...   \n",
       "9999        9999  Fresh Thyme grocery store is giving employees ...   \n",
       "\n",
       "               Sentiment                                         LowerTweet  \\\n",
       "0               Positive  advice talk to your neighbours family to excha...   \n",
       "1               Positive  coronavirus australia woolworths to give elder...   \n",
       "2               Positive  my food stock is not the only one which is emp...   \n",
       "3     Extremely Negative  me ready to go at supermarket during the covid...   \n",
       "4               Positive  as news of the regions first confirmed covid c...   \n",
       "...                  ...                                                ...   \n",
       "9995  Extremely Positive   coronavirus tip shop in your local asian supe...   \n",
       "9996  Extremely Negative  saudi arabia will nearly double its debt ceili...   \n",
       "9997  Extremely Negative  i went to put a few bits in food bank collecti...   \n",
       "9998            Positive  it s a novel experience watching a government ...   \n",
       "9999            Positive  fresh thyme grocery store is giving employees ...   \n",
       "\n",
       "      BooleanSentiment                                     tokenized_text  \\\n",
       "0                    1  ['advice', 'talk', 'to', 'your', 'neighbours',...   \n",
       "1                    1  ['coronavirus', 'australia', 'woolworths', 'to...   \n",
       "2                    1  ['my', 'food', 'stock', 'is', 'not', 'the', 'o...   \n",
       "3                    0  ['me', 'ready', 'to', 'go', 'at', 'supermarket...   \n",
       "4                    1  ['as', 'news', 'of', 'the', 'regions', 'first'...   \n",
       "...                ...                                                ...   \n",
       "9995                 1  ['coronavirus', 'tip', 'shop', 'in', 'your', '...   \n",
       "9996                 0  ['saudi', 'arabia', 'will', 'nearly', 'double'...   \n",
       "9997                 0  ['went', 'to', 'put', 'few', 'bits', 'in', 'fo...   \n",
       "9998                 1  ['it', 'novel', 'experience', 'watching', 'gov...   \n",
       "9999                 1  ['fresh', 'thyme', 'grocery', 'store', 'is', '...   \n",
       "\n",
       "                                         stemmed_tokens  \\\n",
       "0     ['advic', 'talk', 'to', 'your', 'neighbour', '...   \n",
       "1     ['coronaviru', 'australia', 'woolworth', 'to',...   \n",
       "2     ['my', 'food', 'stock', 'is', 'not', 'the', 'o...   \n",
       "3     ['me', 'readi', 'to', 'go', 'at', 'supermarket...   \n",
       "4     ['as', 'new', 'of', 'the', 'region', 'first', ...   \n",
       "...                                                 ...   \n",
       "9995  ['coronaviru', 'tip', 'shop', 'in', 'your', 'l...   \n",
       "9996  ['saudi', 'arabia', 'will', 'nearli', 'doubl',...   \n",
       "9997  ['went', 'to', 'put', 'few', 'bit', 'in', 'foo...   \n",
       "9998  ['it', 'novel', 'experi', 'watch', 'govern', '...   \n",
       "9999  ['fresh', 'thyme', 'groceri', 'store', 'is', '...   \n",
       "\n",
       "                                                 vector  NumberOfWords  \n",
       "0     [593, 452, 1, 32, 1470, 184, 1, 2061, 805, 388...             37  \n",
       "1     [8, 674, 1499, 1, 220, 315, 962, 1324, 33, 209...             16  \n",
       "2     [38, 16, 60, 10, 34, 0, 118, 12, 203, 10, 183,...             42  \n",
       "3     [88, 688, 1, 40, 19, 21, 66, 0, 3, 163, 34, 10...             36  \n",
       "4     [27, 80, 4, 0, 1357, 210, 864, 3, 251, 862, 45...             37  \n",
       "...                                                 ...            ...  \n",
       "9995  [8, 368, 33, 7, 32, 110, 1119, 21, 525, 108, 1...             28  \n",
       "9996  [763, 1145, 36, 1228, 790, 14, 834, 5912, 1, 1...             41  \n",
       "9997  [253, 1, 190, 298, 684, 7, 16, 181, 731, 19, 0...             45  \n",
       "9998    [14, 1059, 707, 407, 168, 67, 4379, 7, 378, 59]             10  \n",
       "9999  [567, 26, 18, 10, 220, 157, 424, 2189, 17, 3, ...             11  \n",
       "\n",
       "[10000 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_y)/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_y)/len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/CNN1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7774, 58, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = OH_vector([3, 10, 34, 248, 881, 14, 107])\n",
    "sample = np.expand_dims(sample, axis=0)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8567021]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
